{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611567dbec5f4dc6",
   "metadata": {},
   "source": [
    "# Alzheimer textual explanation, visual explanation and classification\n",
    "In this notebook there's all the procedure we do for the classification and for the explanation.\n",
    "\n",
    "For the realization of this project i start from the code of my colleague.\n",
    "\n",
    "In this notebook we suppose that you have already the dataset and the explanation, if else, \n",
    "you will run \"Creation of the dataset\" before this notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "919f80c27494ade4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:07.683759Z",
     "start_time": "2024-07-01T09:52:16.568468Z"
    }
   },
   "source": [
    "import os, random, glob, cv2\n",
    "import nltk\n",
    " \n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sys import platform\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical, plot_model \n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense,\n",
    "    LSTM, Embedding,\n",
    "    Dropout, add,\n",
    "    MaxPool3D, Conv3D,\n",
    "    GlobalAveragePooling3D, BatchNormalization\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import importlib\n",
    "import Utility\n",
    "importlib.reload(Utility)\n",
    "from Utility import get_gradcam\n",
    "from alzheimer_disease.src.helpers.utils import get_device\n",
    "from alzheimer_disease.src.modules.training import training_model\n",
    "from alzheimer_disease.src.helpers.config import get_config\n",
    "from alzheimer_disease.src.modules.preprocessing import get_transformations\n",
    "from alzheimer_disease.src.models.densenetmm import DenseNetMM\n",
    "\n",
    "#nltk.download('punkt')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "63f4224939e588e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:07.764176Z",
     "start_time": "2024-07-01T09:53:07.689330Z"
    }
   },
   "source": [
    "# Definition of all paths\n",
    "dataset = 'oasis_aug'\n",
    "\n",
    "_base_path = '/Volumes/Seagate Bas/Vito/CV'\n",
    "_config = get_config()\n",
    "saved_path = os.path.join(_base_path, _config.get('SAVED_FOLDER'))\n",
    "reports_path = os.path.join(_base_path, _config.get('REPORT_FOLDER'))\n",
    "logs_path = os.path.join(_base_path, _config.get('LOG_FOLDER'))\n",
    "_data_path = os.path.join(_base_path, _config.get('LOCAL_DATA'))\n",
    "data_path, meta_path, explanation_path = [\n",
    "    os.path.join(_data_path, dataset, 'data/'),\n",
    "    os.path.join(_data_path, dataset, 'meta/'),\n",
    "    os.path.join(_data_path, dataset, 'explainability/')\n",
    "]\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "if platform == 'win32':\n",
    "    saved_path = saved_path.replace('/', '\\\\')\n",
    "    reports_path = reports_path.replace('/', '\\\\')\n",
    "    logs_path = logs_path.replace('/', '\\\\')\n",
    "    data_path = data_path.replace('/', '\\\\')\n",
    "    meta_path = meta_path.replace('/', '\\\\')\n",
    "    explanation_path = explanation_path.replace('/', '\\\\')\n",
    "\n",
    "saved_path, reports_path, logs_path, data_path, meta_path, explanation_path, device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Volumes/Seagate Bas/Vito/CV/saved/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/reports/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/logs/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/data/oasis_aug/data/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/data/oasis_aug/meta/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/data/oasis_aug/explainability/',\n",
       " 'cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1abdcaaf50e4b278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:07.766989Z",
     "start_time": "2024-07-01T09:53:07.765039Z"
    }
   },
   "source": [
    "SIZE = 128\n",
    "output_length = 1024\n",
    "epochs = 30\n",
    "name_model = 'DenseNetMM_best'\n",
    "\n",
    "CHANNELS = ['T2w']\n",
    "\n",
    "FEATURES = ['sex', 'age', 'bmi', 'education', 'cdr_memory', 'cdr_orientation', 'cdr_judgment', 'cdr_community', 'cdr_hobbies', 'cdr_personalcare', 'boston_naming_test', 'depression', 'sleeping_disorder', 'motor_disturbance']\n",
    "MULTICLASS = True\n",
    "train_transform, eval_transform = get_transformations(size=SIZE)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7c21fb3c080ba3ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:07.780189Z",
     "start_time": "2024-07-01T09:53:07.768361Z"
    }
   },
   "source": [
    "# I started with the train test split of colleague and adapt to my task\n",
    "def train_test_splitting(\n",
    "        data_folder,\n",
    "        meta_folder,\n",
    "        explanation_folder,\n",
    "        channels,\n",
    "        features,\n",
    "        train_ratio=.8,\n",
    "        multiclass=False,\n",
    "        verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Splitting train/eval/test.\n",
    "    Args:\n",
    "        data_folder (str): path of the folder containing images.\n",
    "        meta_folder (str): path of the folder containing csv files.\n",
    "        explanation_folder (str): path of the folder containing csv files of the explanation.\n",
    "        channels (list): image channels to select (values `T1w`, `T2w` or both).\n",
    "        features (list): features set to select.\n",
    "        train_ratio (float): ratio of the training set, value between 0 and 1.\n",
    "        multiclass (bool): `False` for binary classification, `True` for ternary classification.\n",
    "        verbose (bool): whether or not print information.\n",
    "    Returns:\n",
    "        train_data (list): the training data ready to feed monai.data.Dataset\n",
    "        eval_data (list): the evaluation data ready to feed monai.data.Dataset\n",
    "        test_data (list): the testing data ready to feed monai.data.Dataset.\n",
    "        (see https://docs.monai.io/en/latest/data.html#monai.data.Dataset).\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    df = pd.read_csv(os.path.join(meta_folder, 'data_num.csv'))\n",
    "    df1 = df[(df['weight'] != .0) & (df['height'] != .0)]\n",
    "    df['bmi'] = round(df1['weight'] / (df1['height'] * df1['height']), 0)\n",
    "    df['bmi'] = df['bmi'].fillna(.0)\n",
    "    sessions = [s.split('_')[0] for s in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, s))]\n",
    "    subjects = list(set(sessions))\n",
    "\n",
    "    # uploading of the dataset\n",
    "    explanation = pd.read_csv(explanation_folder + 'explaination.csv', sep=';')\n",
    "\n",
    "\n",
    "\n",
    "    # applying splitting on subjects to prevent data leakage\n",
    "    random.shuffle(subjects)\n",
    "    split_train = int(len(subjects) * train_ratio)\n",
    "    train_subjects, test_subjects = subjects[:split_train], subjects[split_train:]\n",
    "    split_eval = int(len(train_subjects) * .8)\n",
    "    eval_subjects = train_subjects[split_eval:]\n",
    "    train_subjects = train_subjects[:split_eval]\n",
    "\n",
    "    # applying multiclass label correction and splitting\n",
    "    if multiclass:\n",
    "        train_subjects, eval_subjects, test_subjects = [], [], []\n",
    "        df.loc[df['cdr'] == .0, 'final_dx'] = .0\n",
    "        df.loc[df['cdr'] == .5, 'final_dx'] = 1.\n",
    "        df.loc[(df['cdr'] != .0) & (df['cdr'] != .5), 'final_dx'] = 2.\n",
    "        m = np.min(np.unique(df['final_dx'].to_numpy(), return_counts=True)[1])\n",
    "        df = pd.concat([\n",
    "            df[df['final_dx'] == .0].sample(m),\n",
    "            df[df['final_dx'] == 1.].sample(m),\n",
    "            df[df['final_dx'] == 2.].sample(m)\n",
    "        ], ignore_index=True)\n",
    "        n_test = m - int(m * train_ratio)\n",
    "        n_eval = m - n_test - int(m * train_ratio * train_ratio)\n",
    "        for i in range(3):\n",
    "            sub = list(set(df[df['final_dx'] == float(i)]['subject_id'].to_numpy()))\n",
    "            random.shuffle(sub)\n",
    "            counter = 0\n",
    "            for j in range(len(sub)):\n",
    "                counter += len(df[df['subject_id'] == sub[j]])\n",
    "                if counter <= n_test:\n",
    "                    test_subjects.append(sub[j])\n",
    "                elif counter > n_test and counter <= (n_test + n_eval):\n",
    "                    eval_subjects.append(sub[j])\n",
    "                else:\n",
    "                    train_subjects.append(sub[j])\n",
    "\n",
    "    # loading sessions paths\n",
    "    X_train = df[df['subject_id'].isin(train_subjects)]\n",
    "    X_eval = df[df['subject_id'].isin(eval_subjects)]\n",
    "    X_test = df[df['subject_id'].isin(test_subjects)]\n",
    "    train_sessions = [os.path.join(data_folder, s) for s in X_train['session_id'].values]\n",
    "    eval_sessions = [os.path.join(data_folder, s) for s in X_eval['session_id'].values]\n",
    "    test_sessions = [os.path.join(data_folder, s) for s in X_test['session_id'].values]\n",
    "\n",
    "    # loading explanation of subjects\n",
    "    explanation_train = explanation[explanation['subject_id'].isin(X_train['subject_id'].values)]\n",
    "    explanation_eval = explanation[explanation['subject_id'].isin(X_eval['subject_id'].values)]\n",
    "    explanation_test = explanation[explanation['subject_id'].isin(X_test['subject_id'].values)]\n",
    "\n",
    "    # scaling numerical data in range [0,1]\n",
    "    X_train.loc[:, features] = scaler.fit_transform(X_train[features])\n",
    "    X_eval.loc[:, features] = scaler.fit_transform(X_eval[features])\n",
    "    X_test.loc[:, features] = scaler.fit_transform(X_test[features])\n",
    "\n",
    "    # arranging data in dictionaries\n",
    "    # I will also take the reference session of the explanation and the image\n",
    "    train_data = [dict({\n",
    "        'image': sorted([os.path.join(s, i) for i in os.listdir(s) if any(c in i for c in channels)]),\n",
    "        'data': X_train[X_train['session_id'] == s.split('/')[-1]][features].values[0],\n",
    "        'label': df[df['session_id'] == s.split('/')[-1]]['final_dx'].values[0],\n",
    "        'explanation': explanation_train[explanation_train['session_id'] == s.split('/')[-1]]['explaination'].values[0],\n",
    "        'session_id': s.split('/')[-1]\n",
    "    }) for s in train_sessions]\n",
    "    eval_data = [dict({\n",
    "        'image': sorted([os.path.join(s, i) for i in os.listdir(s) if any(c in i for c in channels)]),\n",
    "        'data': X_eval[X_eval['session_id'] == s.split('/')[-1]][features].values[0],\n",
    "        'label': df[df['session_id'] == s.split('/')[-1]]['final_dx'].values[0],\n",
    "        'explanation': explanation_eval[explanation_eval['session_id']==s.split('/')[-1]]['explaination'].values[0],\n",
    "        'session_id': s.split('/')[-1]\n",
    "    }) for s in eval_sessions]\n",
    "    test_data = [dict({\n",
    "        'image': sorted([os.path.join(s, i) for i in os.listdir(s) if any(c in i for c in channels)]),\n",
    "        'data': X_test[X_test['session_id'] == s.split('/')[-1]][features].values[0],\n",
    "        'label': df[df['session_id'] == s.split('/')[-1]]['final_dx'].values[0],\n",
    "        'explanation': explanation_test[explanation_test['session_id'] == s.split('/')[-1]]['explaination'].values[0],\n",
    "        'session_id': s.split('/')[-1]\n",
    "    }) for s in test_sessions]\n",
    "\n",
    "    # print data splitting information\n",
    "    if verbose:\n",
    "        print(''.join(['> ' for _ in range(40)]))\n",
    "        print(f'\\n{\"\":<20}{\"TRAINING\":<20}{\"EVALUATION\":<20}{\"TESTING\":<20}\\n')\n",
    "        print(''.join(['> ' for _ in range(40)]))\n",
    "        tsb1 = str(len(train_subjects)) + ' (' + str(round((len(train_subjects) * 100 / len(df['subject_id'].unique())), 0)) + ' %)'\n",
    "        tsb2 = str(len(eval_subjects)) + ' (' + str(round((len(eval_subjects) * 100 / len(df['subject_id'].unique())), 0)) + ' %)'\n",
    "        tsb3 = str(len(test_subjects)) + ' (' + str(round((len(test_subjects) * 100 / len(df['subject_id'].unique())), 0)) + ' %)'\n",
    "        tss1 = str(len(train_sessions)) + ' (' + str(round((len(train_sessions) * 100 / len(df)), 2)) + ' %)'\n",
    "        tss2 = str(len(eval_sessions)) + ' (' + str(round((len(eval_sessions) * 100 / len(df)), 2)) + ' %)'\n",
    "        tss3 = str(len(test_sessions)) + ' (' + str(round((len(test_sessions) * 100 / len(df)), 2)) + ' %)'\n",
    "        print(f'\\n{\"subjects\":<20}{tsb1:<20}{tsb2:<20}{tsb3:<20}\\n')\n",
    "        print(f'{\"sessions\":<20}{tss1:<20}{tss2:<20}{tss3:<20}\\n')\n",
    "\n",
    "    return train_data, eval_data, test_data"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "72b25c026b7fb31b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:07.967396Z",
     "start_time": "2024-07-01T09:53:07.781Z"
    }
   },
   "source": [
    "densenet = DenseNetMM(\n",
    "    in_channels = len(CHANNELS),\n",
    "    in_size = SIZE,\n",
    "    in_features_size= len(FEATURES),\n",
    "    out_channels = 3 if MULTICLASS else 2,\n",
    "    append_features = True,\n",
    "    name=name_model\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6b704a8642ecf907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:10.455704Z",
     "start_time": "2024-07-01T09:53:07.968192Z"
    }
   },
   "source": [
    "train_transform, eval_transform = get_transformations(size=SIZE)\n",
    "\n",
    "train, val, test = train_test_splitting(\n",
    "    data_folder=data_path,\n",
    "    meta_folder=meta_path,\n",
    "    explanation_folder=explanation_path,\n",
    "    channels=CHANNELS,\n",
    "    features=FEATURES,\n",
    "    multiclass=MULTICLASS,\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \n",
      "\n",
      "                    TRAINING            EVALUATION          TESTING             \n",
      "\n",
      "> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \n",
      "\n",
      "subjects            371 (63.0 %)        95 (16.0 %)         122 (21.0 %)        \n",
      "\n",
      "sessions            435 (63.6 %)        111 (16.23 %)       138 (20.18 %)       \n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a143df277ef4f5dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:10.459106Z",
     "start_time": "2024-07-01T09:53:10.456450Z"
    }
   },
   "source": [
    "train[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': ['/Volumes/Seagate Bas/Vito/CV/data/oasis_aug/data/OAS31462_MR_d0107/sub-OAS31462_sess-d0107_acq-TSE_T2w.nii.gz'],\n",
       " 'data': array([1.        , 0.48      , 0.69565217, 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.5       , 0.5       , 0.5       ]),\n",
       " 'label': 0.0,\n",
       " 'explanation': \"**Summary of Heatmap Analysis**\\n\\nThe heatmap analysis highlights several regions in the brain that the classifier model focused on to make its classification decision for Alzheimer's Disease detection. The regions highlighted are not areas affected by the disease, but rather areas that the model considered important for its decision-making process.\\n\\n1. **Frontal-to-Occipital (GapMap) left**: The heatmap covers 19.9% of this region, which corresponds to 17.81% of the region affected by Alzheimer's Disease. This region is responsible for processing visual information and is involved in attention and memory tasks. The model's focus on this region may indicate that it is searching for patterns related to visual processing or attentional deficits, which are common symptoms of Alzheimer's Disease.\\n2. **Area hOc1 (V1, 17, CalcS) left**: The heatmap covers 13.24% of this region, which corresponds to 43.39% of the region affected by Alzheimer's Disease. This region is involved in processing visual information, particularly in the early stages of visual processing. The model's focus on this region may suggest that it is looking for patterns related to visual processing or early-stage visual impairments, which could be indicative of Alzheimer's Disease.\\n3. **Temporal-to-Parietal (GapMap) left**: The heatmap covers 12.87% of this region, which corresponds to 7.68% of the region affected by Alzheimer's Disease. This region is responsible for processing auditory information and is involved in attention and memory tasks. The model's focus on this region may indicate that it is searching for patterns related to auditory processing or attentional deficits, which are common symptoms of Alzheimer's Disease.\\n4. **Area hOc4v (LingG) left**: The heatmap covers 8.06% of this region, which corresponds to 68.77% of the region affected by Alzheimer's Disease. This region is involved in processing linguistic information and is responsible for language processing and memory tasks. The model's focus on this region may suggest that it is looking for patterns related to language processing or memory impairments, which are common symptoms of Alzheimer's Disease.\\n5. **Area hOc3v (LingG) left**: The heatmap covers 6.54% of this region, which corresponds to 51.49% of the region affected by Alzheimer's Disease. This region is also involved in processing linguistic information and is responsible for language processing and memory tasks. The model's focus on this region may indicate that it is searching for patterns related to language processing or memory impairments, which are common symptoms of Alzheimer's Disease.\\n\\n**Conclusion**\\n\\nThe heatmap analysis provides valuable insights into the regions of the brain that the classifier model focused on to make its classification decision for Alzheimer's Disease detection. The regions highlighted are not areas affected by the disease, but rather areas that the model considered important for its decision-making process.\\n\\nThe analysis suggests that the model is searching for patterns related to visual processing, attentional deficits, auditory processing, language processing, and memory impairments, which are common symptoms of Alzheimer's Disease. This information can enhance clinical decision-making and potentially reveal new aspects of Alzheimer's Disease pathology and diagnosis.\\n\\nThe model's focus on these regions may encourage further investigation into the signs of Alzheimer's Disease, such as visual processing deficits, attentional impairments, and language processing difficulties. This information can help clinicians develop targeted diagnostic and therapeutic strategies for patients with Alzheimer's Disease.\",\n",
       " 'session_id': 'OAS31462_MR_d0107'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "c7d109ec7057aa1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:12.079526Z",
     "start_time": "2024-07-01T09:53:10.460290Z"
    }
   },
   "source": [
    "if glob.glob(saved_path+ f'{name_model}.pth'):\n",
    "\tprint(f'Loading {name_model}.pth')\n",
    "\tdensenet.load_state_dict(torch.load(saved_path + f'{name_model}.pth'))\n",
    "else:\n",
    "\tprint('Train of the model')\n",
    "\ttrain_metrics = training_model(\n",
    "\t\tmodel = densenet,\n",
    "\t\tdata = [train, val],\n",
    "\t\ttransforms = [train_transform, eval_transform],\n",
    "\t\tepochs = epochs,\n",
    "\t\tdevice = get_device(),\n",
    "\t\tpaths = [saved_path, reports_path, logs_path],\n",
    "\t\tnum_workers=0,\n",
    "\t\tverbose=True\n",
    "\t)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DenseNetMM_best.pth\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "151e3073d8cfb812",
   "metadata": {},
   "source": [
    "## Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "id": "36632317f8234359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:12.082035Z",
     "start_time": "2024-07-01T09:53:12.080192Z"
    }
   },
   "source": [
    "name_fextractor = 'DenseNetMMFeatureExtractor'"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "abeaaea064344a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:12.174442Z",
     "start_time": "2024-07-01T09:53:12.084428Z"
    }
   },
   "source": [
    "from monai.data import CacheDataset\n",
    "\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    re1 = re.compile(r'  +')\n",
    "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '', \"\\n\").replace('\\\"', '\"').replace('', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\'', ' \\ ')\n",
    "    return re1.sub(' ', html.unescape(x1))\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def replace_numbers(text):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_stopwords(words, stop_words):\n",
    "    \"\"\"\n",
    "    :param words:\n",
    "    :type words:\n",
    "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    or\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    :type stop_words:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def addsequences(text):\n",
    "  return '' .join('startseq ' + \" \".join([word for word in text.split() if len(word)>1]) + ' endseq')\n",
    "\n",
    "def text2words(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def normalize_text( text):\n",
    "    #text = remove_special_chars(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = replace_numbers(text)\n",
    "    text = addsequences(text)\n",
    "    return text\n",
    "  \n",
    "def normalize_corpus(corpus):\n",
    "    return [normalize_text(t) for t in corpus]\n",
    "\n",
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# concat for get all df\n",
    "entire_df = train + val + test\n",
    "\n",
    "all_text = [explaination['explanation'] for explaination in entire_df]\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(all_text))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(vocab_size))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1174\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a73cbe58bf77e466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:12.375008Z",
     "start_time": "2024-07-01T09:53:12.175407Z"
    }
   },
   "source": [
    "feature_extractor = DenseNetMM(\n",
    "    in_channels = len(CHANNELS),\n",
    "    in_size = SIZE,\n",
    "    in_features_size= len(FEATURES),\n",
    "    out_channels = 3 if MULTICLASS else 2,\n",
    "    append_features = True,\n",
    "    name=name_fextractor\n",
    ")\n",
    "\n",
    "# Upload the previous model for the feature extraction\n",
    "if glob.glob(saved_path+ f'{name_model}.pth'):\n",
    "\tprint(f'Loading {name_model}.pth')\n",
    "\tfeature_extractor.load_state_dict(torch.load(saved_path + f'{name_model}.pth'))\n",
    "else:\n",
    "\tprint('Train of the model')\n",
    "\ttrain_metrics = training_model(\n",
    "\t\tmodel = feature_extractor,\n",
    "\t\tdata = [train, val],\n",
    "\t\ttransforms = [train_transform, eval_transform],\n",
    "\t\tepochs = epochs,\n",
    "\t\tdevice = get_device(),\n",
    "\t\tpaths = [saved_path, reports_path, logs_path],\n",
    "\t\tnum_workers=0,\n",
    "\t\tverbose=True\n",
    "\t)\n",
    "    \n",
    "# get just the feature extractor from image"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DenseNetMM_best.pth\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:12.381585Z",
     "start_time": "2024-07-01T09:53:12.376042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_extractor = torch.nn.Sequential(\n",
    "    feature_extractor.features_img,\n",
    "    feature_extractor.output_layers,\n",
    "    #nn.Linear(1024, 256)\n",
    ")\n",
    "\n",
    "feature_extractor"
   ],
   "id": "2dd69abddf8241d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (conv0): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
       "    (norm0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(96, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(160, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(192, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(224, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(160, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(192, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(224, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(288, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(320, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(352, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(384, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(416, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(448, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(480, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(288, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(320, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(352, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(384, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(416, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(448, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(480, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(544, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(576, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(608, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(640, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(672, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(704, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(736, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(768, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(800, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(832, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(864, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(896, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(928, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(960, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(992, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(544, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(576, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(608, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(640, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(672, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(704, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(736, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(768, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(800, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(832, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(864, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(896, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(928, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(960, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (layers): Sequential(\n",
       "          (norm1): BatchNorm3d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv3d(992, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (pool): AdaptiveAvgPool3d(output_size=1)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "37df97b1f82294ab",
   "metadata": {},
   "source": [
    "cache_entire_df = CacheDataset(entire_df, transform=train_transform, cache_rate=1.0, num_workers=None, progress=False)\n",
    "entire_df_loader = DataLoader(cache_entire_df, batch_size=1, shuffle=False, num_workers=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c3f2343da09757e",
   "metadata": {},
   "source": [
    "# extract features from image\n",
    "features = {}\n",
    "\n",
    "for img_name in tqdm(entire_df_loader):\n",
    "    image, val_inputs_data, session_id  = (\n",
    "        img_name['image'],\n",
    "        img_name['data'],\n",
    "        img_name['session_id']\n",
    "    )\n",
    "    # extract features\n",
    "    feature = feature_extractor(image)\n",
    "    # store feature\n",
    "    features[session_id[0]] = feature[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bd47997a581c0fc",
   "metadata": {},
   "source": [
    "pickle.dump(features, open(os.path.join(saved_path, 'clef_features_vgg.pkl'), 'wb'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "828b9d754e572382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:12.577363Z",
     "start_time": "2024-07-01T09:53:12.382288Z"
    }
   },
   "source": [
    "if glob.glob(saved_path + 'clef_features_vgg.pkl'):\n",
    "    with open(os.path.join(saved_path, 'clef_features_vgg.pkl'), 'rb') as f:\n",
    "        print('Loading clef_features_vgg.pkl')\n",
    "        features = pickle.load(f)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clef_features_vgg.pkl\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "b9b7e473db449f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:12.672953Z",
     "start_time": "2024-07-01T09:53:12.578367Z"
    }
   },
   "source": [
    "# encoder model\n",
    "# image feature layers\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(output_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "generation_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "generation_model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=[\"accuracy\"])\n",
    "\n",
    "# plot the model\n",
    "plot_model(generation_model, show_shapes=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "ff6da244206dca5c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# create data generator to get data in batch (avoids session crash)\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            feature_key = key['session_id']\n",
    "            captions = mapping[key['session_id']]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    # store the sequences\n",
    "                    #print([key])\n",
    "                    X1.append(features[feature_key])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                yield (np.asarray(X1), np.asarray(X2)), np.asarray(y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:53:12.762442Z",
     "start_time": "2024-07-01T09:53:12.678015Z"
    }
   },
   "cell_type": "code",
   "source": "mapping = {key['session_id']: normalize_text(key['explanation']) for key in entire_df}",
   "id": "eaa1e4296e419d97",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-01T09:53:12.763212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if glob.glob(saved_path + 'model_generation.h5'):\n",
    "    print('Loading generation_model.h5')\n",
    "    generation_model.load_weights(saved_path + 'model_generation.h5')\n",
    "else:\n",
    "    # train the model\n",
    "    batch_size = 32\n",
    "    steps = len(train) // batch_size\n",
    "    \n",
    "    train_generator = data_generator(train, mapping, features, tokenizer, output_length, vocab_size, batch_size)\n",
    "    val_generator = data_generator(val, mapping, features, tokenizer, output_length, vocab_size, batch_size)\n",
    "    # fit for 10 epoch\n",
    "    generation_model.fit(train_generator, epochs=epochs, validation_data=val_generator, steps_per_epoch=steps, verbose=1)\n",
    "    generation_model.save(saved_path+'model_generation.h5')"
   ],
   "id": "fe142bc1fe6e03b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "818a35cc7dc91324",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "      \n",
    "    return in_text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc745e0cb956b4b9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "all_captions = []\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    feature_key = key.split('.')[0]\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(generation_model, features[feature_key], tokenizer, output_length)\n",
    "    all_captions.append(y_pred)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "    \n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "print(all_captions[0]) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "efe0392784b62bca",
   "metadata": {},
   "source": [
    "# Merging of the proposed methods"
   ]
  },
  {
   "cell_type": "code",
   "id": "18a2908249a121ad",
   "metadata": {},
   "source": [
    "def plot_grad_cam_explanation(image, label, pred, heatmap, mask, caption, alpha=128):\n",
    "\t\"\"\"\n",
    "\tPlots model input image, Grad-CAM heatmap, segmentation mask and the explanation generated\n",
    "\tArgs:\n",
    "\t\timage (numpy.ndarray): the input 3D image.\n",
    "\t\tlabel (int): the input image label.\n",
    "\t\tpred (int): model prediction for input image.\n",
    "\t\theatmap (numpy.ndarray): the Grad-CAM 3D heatmap.\n",
    "\t\tmask (numpy.ndarray): the computed 3D segmentation mask.\n",
    "\t\tcaption (string): the explanation generated caption.\n",
    "\t\talpha (int): transparency channel. Between 0 and 255.\n",
    "\tReturns:\n",
    "\t\tNone.\n",
    "\t\"\"\"\n",
    "\tif alpha >= 0 and alpha <= 255:\n",
    "\t\theatmap_mask = np.zeros((image.shape[0], image.shape[1], image.shape[2], 4), dtype='uint8')\n",
    "\t\theatmap_mask[mask == 1] = [255, 0, 0, alpha]\n",
    "\t\timage = image[:,:,int(image.shape[2] / 2)]\n",
    "\t\theatmap = heatmap[:,:,int(heatmap.shape[2] / 2)]\n",
    "\t\theatmap_mask = heatmap_mask[:,:,int(heatmap_mask.shape[2] / 2),:]\n",
    "\t\tfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\t\tnorm_img = cv2.normalize(image, np.zeros((image.shape[1], image.shape[0])), 0, 1, cv2.NORM_MINMAX)\n",
    "\t\tim_shows = [\n",
    "\t\t\taxs[0].imshow(norm_img, cmap='gray', interpolation='bilinear', vmin = .0, vmax = 1.),\n",
    "\t\t\taxs[1].imshow(heatmap, cmap='jet', interpolation='bilinear', vmin = .0, vmax = 1.),\n",
    "\t\t\taxs[2].imshow(norm_img, cmap='gray', interpolation='bilinear', vmin = .0, vmax = 1.)\n",
    "\t\t]\n",
    "\t\taxs[2].imshow(heatmap_mask, interpolation='bilinear')\n",
    "\t\taxs[0].set_title('Label=' + ('NON-AD' if label == 0 else 'AD') + ' | Prediction=' + ('NON-AD' if pred == 0 else 'AD'), fontsize=16)\n",
    "\t\taxs[1].set_title('Grad-CAM Heatmap', fontsize=16)\n",
    "\t\taxs[2].set_title('Mask - Threshold ' + str(.8), fontsize=16)\n",
    "\t\tfor i, ax in enumerate(axs):\n",
    "\t\t\tax.axis('off')\n",
    "\t\t\tfig.colorbar(im_shows[i], ax=ax, ticks=np.linspace(0,1,6))\n",
    "            \n",
    "        # insert of caption generated\n",
    "        fig.text(0.5, 0.04, caption, ha='center', va='center')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "\t\tprint('\\n' + ''.join(['> ' for i in range(30)]))\n",
    "\t\tprint('\\nERROR: alpha channel \\033[95m '+alpha+'\\033[0m out of range [0,255].\\n')\n",
    "\t\tprint(''.join(['> ' for i in range(30)]) + '\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6dc33693af7c4208",
   "metadata": {},
   "source": [
    "def get_results_and_plot(image_dict, predictor, generator,saved_path, tokenizer, max_length, plot=False):\n",
    "    '''\n",
    "    In this function we predict the class of the image after that we\n",
    "    keep the Grad-CAM and the explanation and return them\n",
    "    :param plot: if True we plot the Grad-CAM and explanation\n",
    "    :param saved_path: directory where the model weights are stored\n",
    "    :param image_dict: image dictionary\n",
    "    :param predictor: model for predict all value\n",
    "    :param generator: model for the generation of explanation\n",
    "    :param tokenizer: tokenizer object\n",
    "    :param max_length: maximum length of explanation\n",
    "    :return: \n",
    "    '''\n",
    "    \n",
    "    # Keep the image, the mask and the prediction\n",
    "    image, mask, pred, label, heatmap = get_gradcam(\n",
    "        example=image_dict,\n",
    "        model=predictor,\n",
    "        saved_path=saved_path,\n",
    "        threshold=.8,\n",
    "    )\n",
    "    \n",
    "    # Generate the description from the processed image\n",
    "    explanation = predict_caption(generator, image, tokenizer, max_length)\n",
    "    \n",
    "    if plot:\n",
    "        plot_grad_cam_explanation(image, label, pred, heatmap, mask, explanation)\n",
    "    \n",
    "    return  image, label, pred, heatmap, mask, explanation"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "156869a8a6cbe018",
   "metadata": {},
   "source": [
    "# get a random example from the entire dataset\n",
    "example = entire_df[random.randint(0, entire_df.shape[0]-1)]\n",
    "get_results_and_plot(\n",
    "    image_dict=example,\n",
    "    predictor=densenet,\n",
    "    generator=generation_model,\n",
    "    saved_path=saved_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=output_length,\n",
    "    plot=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a163c23e063c236",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
