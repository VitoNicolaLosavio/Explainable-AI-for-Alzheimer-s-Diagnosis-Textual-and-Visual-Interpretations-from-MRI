{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611567dbec5f4dc6",
   "metadata": {},
   "source": [
    "# Alzheimer textual explanation, visual explanation and classification\n",
    "In this notebook there's all the procedure we do for the classification and for the explanation.\n",
    "\n",
    "For the realization of this project i start from the code of my colleague.\n",
    "\n",
    "In this notebook we suppose that you have already the dataset and the explanation, if else, \n",
    "you will run \"Creation of the dataset\" before this notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "919f80c27494ade4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:06.088168Z",
     "start_time": "2024-07-01T07:22:53.896380Z"
    }
   },
   "source": [
    "import os, random, glob, cv2\n",
    "import nltk\n",
    " \n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sys import platform\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical, plot_model \n",
    "from monai.data import CacheDataset\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense,\n",
    "    LSTM, Embedding,\n",
    "    Dropout, add,\n",
    "    MaxPool3D, Conv3D,\n",
    "    GlobalAveragePooling3D, BatchNormalization\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import importlib\n",
    "import Utility\n",
    "importlib.reload(Utility)\n",
    "from Utility import get_gradcam\n",
    "from alzheimer_disease.src.helpers.utils import get_device\n",
    "from alzheimer_disease.src.modules.training import training_model\n",
    "from alzheimer_disease.src.helpers.config import get_config\n",
    "from alzheimer_disease.src.modules.preprocessing import get_transformations\n",
    "from alzheimer_disease.src.models.densenetmm import DenseNetMM\n",
    "\n",
    "#nltk.download('punkt')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "63f4224939e588e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:06.132504Z",
     "start_time": "2024-07-01T07:23:06.089481Z"
    }
   },
   "source": [
    "# Definition of all paths\n",
    "dataset = 'oasis_aug'\n",
    "\n",
    "_base_path = '/Volumes/Seagate Bas/Vito/CV'\n",
    "_config = get_config()\n",
    "saved_path = os.path.join(_base_path, _config.get('SAVED_FOLDER'))\n",
    "reports_path = os.path.join(_base_path, _config.get('REPORT_FOLDER'))\n",
    "logs_path = os.path.join(_base_path, _config.get('LOG_FOLDER'))\n",
    "_data_path = os.path.join(_base_path, _config.get('LOCAL_DATA'))\n",
    "data_path, meta_path, explanation_path = [\n",
    "    os.path.join(_data_path, dataset, 'data/'),\n",
    "    os.path.join(_data_path, dataset, 'meta/'),\n",
    "    os.path.join(_data_path, dataset, 'explainability/')\n",
    "]\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "if platform == 'win32':\n",
    "    saved_path = saved_path.replace('/', '\\\\')\n",
    "    reports_path = reports_path.replace('/', '\\\\')\n",
    "    logs_path = logs_path.replace('/', '\\\\')\n",
    "    data_path = data_path.replace('/', '\\\\')\n",
    "    meta_path = meta_path.replace('/', '\\\\')\n",
    "    explanation_path = explanation_path.replace('/', '\\\\')\n",
    "\n",
    "saved_path, reports_path, logs_path, data_path, meta_path, explanation_path, device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Volumes/Seagate Bas/Vito/CV/saved/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/reports/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/logs/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/data/oasis_aug/data/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/data/oasis_aug/meta/',\n",
       " '/Volumes/Seagate Bas/Vito/CV/data/oasis_aug/explainability/',\n",
       " 'cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1abdcaaf50e4b278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:06.135287Z",
     "start_time": "2024-07-01T07:23:06.133093Z"
    }
   },
   "source": [
    "SIZE = 128\n",
    "output_length = 1024\n",
    "epochs = 30\n",
    "name_model = 'DenseNetMM_best'\n",
    "\n",
    "CHANNELS = ['T2w']\n",
    "\n",
    "FEATURES = ['sex', 'age', 'bmi', 'education', 'cdr_memory', 'cdr_orientation', 'cdr_judgment', 'cdr_community', 'cdr_hobbies', 'cdr_personalcare', 'boston_naming_test', 'depression', 'sleeping_disorder', 'motor_disturbance']\n",
    "MULTICLASS = True"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7c21fb3c080ba3ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:06.147990Z",
     "start_time": "2024-07-01T07:23:06.136550Z"
    }
   },
   "source": [
    "# I started with the train test split of colleague and adapt to my task\n",
    "def train_test_splitting(\n",
    "        data_folder,\n",
    "        meta_folder,\n",
    "        explanation_folder,\n",
    "        channels,\n",
    "        features,\n",
    "        train_ratio=.8,\n",
    "        multiclass=False,\n",
    "        verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Splitting train/eval/test.\n",
    "    Args:\n",
    "        data_folder (str): path of the folder containing images.\n",
    "        meta_folder (str): path of the folder containing csv files.\n",
    "        explanation_folder (str): path of the folder containing csv files of the explanation.\n",
    "        channels (list): image channels to select (values `T1w`, `T2w` or both).\n",
    "        features (list): features set to select.\n",
    "        train_ratio (float): ratio of the training set, value between 0 and 1.\n",
    "        multiclass (bool): `False` for binary classification, `True` for ternary classification.\n",
    "        verbose (bool): whether or not print information.\n",
    "    Returns:\n",
    "        train_data (list): the training data ready to feed monai.data.Dataset\n",
    "        eval_data (list): the evaluation data ready to feed monai.data.Dataset\n",
    "        test_data (list): the testing data ready to feed monai.data.Dataset.\n",
    "        (see https://docs.monai.io/en/latest/data.html#monai.data.Dataset).\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    df = pd.read_csv(os.path.join(meta_folder, 'data_num.csv'))\n",
    "    df1 = df[(df['weight'] != .0) & (df['height'] != .0)]\n",
    "    df['bmi'] = round(df1['weight'] / (df1['height'] * df1['height']), 0)\n",
    "    df['bmi'] = df['bmi'].fillna(.0)\n",
    "    sessions = [s.split('_')[0] for s in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, s))]\n",
    "    subjects = list(set(sessions))\n",
    "\n",
    "    # uploading of the dataset\n",
    "    explanation = pd.read_csv(explanation_folder + 'explaination.csv', sep=';')\n",
    "\n",
    "\n",
    "\n",
    "    # applying splitting on subjects to prevent data leakage\n",
    "    random.shuffle(subjects)\n",
    "    split_train = int(len(subjects) * train_ratio)\n",
    "    train_subjects, test_subjects = subjects[:split_train], subjects[split_train:]\n",
    "    split_eval = int(len(train_subjects) * .8)\n",
    "    eval_subjects = train_subjects[split_eval:]\n",
    "    train_subjects = train_subjects[:split_eval]\n",
    "\n",
    "    # applying multiclass label correction and splitting\n",
    "    if multiclass:\n",
    "        train_subjects, eval_subjects, test_subjects = [], [], []\n",
    "        df.loc[df['cdr'] == .0, 'final_dx'] = .0\n",
    "        df.loc[df['cdr'] == .5, 'final_dx'] = 1.\n",
    "        df.loc[(df['cdr'] != .0) & (df['cdr'] != .5), 'final_dx'] = 2.\n",
    "        m = np.min(np.unique(df['final_dx'].to_numpy(), return_counts=True)[1])\n",
    "        df = pd.concat([\n",
    "            df[df['final_dx'] == .0].sample(m),\n",
    "            df[df['final_dx'] == 1.].sample(m),\n",
    "            df[df['final_dx'] == 2.].sample(m)\n",
    "        ], ignore_index=True)\n",
    "        n_test = m - int(m * train_ratio)\n",
    "        n_eval = m - n_test - int(m * train_ratio * train_ratio)\n",
    "        for i in range(3):\n",
    "            sub = list(set(df[df['final_dx'] == float(i)]['subject_id'].to_numpy()))\n",
    "            random.shuffle(sub)\n",
    "            counter = 0\n",
    "            for j in range(len(sub)):\n",
    "                counter += len(df[df['subject_id'] == sub[j]])\n",
    "                if counter <= n_test:\n",
    "                    test_subjects.append(sub[j])\n",
    "                elif counter > n_test and counter <= (n_test + n_eval):\n",
    "                    eval_subjects.append(sub[j])\n",
    "                else:\n",
    "                    train_subjects.append(sub[j])\n",
    "\n",
    "    # loading sessions paths\n",
    "    X_train = df[df['subject_id'].isin(train_subjects)]\n",
    "    X_eval = df[df['subject_id'].isin(eval_subjects)]\n",
    "    X_test = df[df['subject_id'].isin(test_subjects)]\n",
    "    train_sessions = [os.path.join(data_folder, s) for s in X_train['session_id'].values]\n",
    "    eval_sessions = [os.path.join(data_folder, s) for s in X_eval['session_id'].values]\n",
    "    test_sessions = [os.path.join(data_folder, s) for s in X_test['session_id'].values]\n",
    "\n",
    "    # loading explanation of subjects\n",
    "    explanation_train = explanation[explanation['subject_id'].isin(X_train['subject_id'].values)]\n",
    "    explanation_eval = explanation[explanation['subject_id'].isin(X_eval['subject_id'].values)]\n",
    "    explanation_test = explanation[explanation['subject_id'].isin(X_test['subject_id'].values)]\n",
    "\n",
    "    # scaling numerical data in range [0,1]\n",
    "    X_train.loc[:, features] = scaler.fit_transform(X_train[features])\n",
    "    X_eval.loc[:, features] = scaler.fit_transform(X_eval[features])\n",
    "    X_test.loc[:, features] = scaler.fit_transform(X_test[features])\n",
    "\n",
    "    # arranging data in dictionaries\n",
    "    # I will also take the reference session of the explanation and the image\n",
    "    train_data = [dict({\n",
    "        'image': sorted([os.path.join(s, i) for i in os.listdir(s) if any(c in i for c in channels)]),\n",
    "        'data': X_train[X_train['session_id'] == s.split('/')[-1]][features].values[0],\n",
    "        'label': df[df['session_id'] == s.split('/')[-1]]['final_dx'].values[0],\n",
    "        'explanation': explanation_train[explanation_train['session_id'] == s.split('/')[-1]]['explaination'].values[0],\n",
    "        'session_id': s.split('/')[-1]\n",
    "    }) for s in train_sessions]\n",
    "    eval_data = [dict({\n",
    "        'image': sorted([os.path.join(s, i) for i in os.listdir(s) if any(c in i for c in channels)]),\n",
    "        'data': X_eval[X_eval['session_id'] == s.split('/')[-1]][features].values[0],\n",
    "        'label': df[df['session_id'] == s.split('/')[-1]]['final_dx'].values[0],\n",
    "        'explanation': explanation_eval[explanation_eval['session_id']==s.split('/')[-1]]['explaination'].values[0],\n",
    "        'session_id': s.split('/')[-1]\n",
    "    }) for s in eval_sessions]\n",
    "    test_data = [dict({\n",
    "        'image': sorted([os.path.join(s, i) for i in os.listdir(s) if any(c in i for c in channels)]),\n",
    "        'data': X_test[X_test['session_id'] == s.split('/')[-1]][features].values[0],\n",
    "        'label': df[df['session_id'] == s.split('/')[-1]]['final_dx'].values[0],\n",
    "        'explanation': explanation_test[explanation_test['session_id'] == s.split('/')[-1]]['explaination'].values[0],\n",
    "        'session_id': s.split('/')[-1]\n",
    "    }) for s in test_sessions]\n",
    "\n",
    "    # print data splitting information\n",
    "    if verbose:\n",
    "        print(''.join(['> ' for _ in range(40)]))\n",
    "        print(f'\\n{\"\":<20}{\"TRAINING\":<20}{\"EVALUATION\":<20}{\"TESTING\":<20}\\n')\n",
    "        print(''.join(['> ' for _ in range(40)]))\n",
    "        tsb1 = str(len(train_subjects)) + ' (' + str(round((len(train_subjects) * 100 / len(df['subject_id'].unique())), 0)) + ' %)'\n",
    "        tsb2 = str(len(eval_subjects)) + ' (' + str(round((len(eval_subjects) * 100 / len(df['subject_id'].unique())), 0)) + ' %)'\n",
    "        tsb3 = str(len(test_subjects)) + ' (' + str(round((len(test_subjects) * 100 / len(df['subject_id'].unique())), 0)) + ' %)'\n",
    "        tss1 = str(len(train_sessions)) + ' (' + str(round((len(train_sessions) * 100 / len(df)), 2)) + ' %)'\n",
    "        tss2 = str(len(eval_sessions)) + ' (' + str(round((len(eval_sessions) * 100 / len(df)), 2)) + ' %)'\n",
    "        tss3 = str(len(test_sessions)) + ' (' + str(round((len(test_sessions) * 100 / len(df)), 2)) + ' %)'\n",
    "        print(f'\\n{\"subjects\":<20}{tsb1:<20}{tsb2:<20}{tsb3:<20}\\n')\n",
    "        print(f'{\"sessions\":<20}{tss1:<20}{tss2:<20}{tss3:<20}\\n')\n",
    "\n",
    "    return train_data, eval_data, test_data"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "72b25c026b7fb31b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:06.311307Z",
     "start_time": "2024-07-01T07:23:06.148631Z"
    }
   },
   "source": [
    "densenet = DenseNetMM(\n",
    "    in_channels = len(CHANNELS),\n",
    "    in_size = SIZE,\n",
    "    in_features_size= len(FEATURES),\n",
    "    out_channels = 3 if MULTICLASS else 2,\n",
    "    append_features = True,\n",
    "    name=name_model\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6b704a8642ecf907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:17.381980Z",
     "start_time": "2024-07-01T07:23:06.312079Z"
    }
   },
   "source": [
    "train_transform, eval_transform = get_transformations(size=SIZE)\n",
    "\n",
    "train, val, test = train_test_splitting(\n",
    "    data_folder=data_path,\n",
    "    meta_folder=meta_path,\n",
    "    explanation_folder=explanation_path,\n",
    "    channels=CHANNELS,\n",
    "    features=FEATURES,\n",
    "    multiclass=MULTICLASS,\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \n",
      "\n",
      "                    TRAINING            EVALUATION          TESTING             \n",
      "\n",
      "> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \n",
      "\n",
      "subjects            380 (65.0 %)        98 (17.0 %)         111 (19.0 %)        \n",
      "\n",
      "sessions            435 (63.6 %)        111 (16.23 %)       138 (20.18 %)       \n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a143df277ef4f5dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:17.385398Z",
     "start_time": "2024-07-01T07:23:17.382693Z"
    }
   },
   "source": [
    "train[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': ['/Volumes/Seagate Bas/Vito/CV/data/oasis_aug/data/OAS30788_MR_d4108/sub-OAS30788_sess-d4108_acq-TSE_T2w.nii.gz'],\n",
       " 'data': array([1.        , 0.8       , 0.47826087, 0.66666667, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.96666667, 0.5       , 0.5       , 0.5       ]),\n",
       " 'label': 0.0,\n",
       " 'explanation': \"**Summary of Heatmap Analysis**\\n\\nThe heatmap analysis reveals that the machine learning model focused on specific regions of the brain to make its classification decision. The highlighted regions are not areas affected by Alzheimer's Disease, but rather areas that the model considered crucial for its prediction.\\n\\n1. **Frontal-to-Occipital (GapMap) left**: This region accounts for 19.9% of the heatmap. According to the Julich-Brain Atlas, this area is responsible for processing visual information and is involved in attention and executive functions. The model's focus on this region may indicate that it is considering the patient's cognitive abilities, such as attention and memory, in its classification decision. This could suggest that the model is looking for signs of cognitive decline, which is a hallmark of Alzheimer's Disease.\\n2. **Area hOc1 (V1, 17, CalcS) left**: This region occupies 13.24% of the heatmap. This area is part of the primary visual cortex, responsible for processing basic visual features such as line orientation and color. The model's focus on this region may imply that it is examining the patient's visual processing abilities, which could be affected in the early stages of Alzheimer's Disease.\\n3. **Temporal-to-Parietal (GapMap) left**: This region accounts for 12.87% of the heatmap. This area is involved in processing auditory information and is also responsible for attention and memory. The model's focus on this region may suggest that it is considering the patient's auditory processing abilities, which could be impaired in Alzheimer's Disease.\\n4. **Area hOc4v (LingG) left**: This region occupies 8.06% of the heatmap. This area is part of the lateral occipital complex, involved in processing visual information and recognizing objects. The model's focus on this region may indicate that it is examining the patient's ability to recognize and process visual stimuli, which could be affected in Alzheimer's Disease.\\n5. **Area hOc3v (LingG) left**: This region accounts for 6.54% of the heatmap. This area is also part of the lateral occipital complex, involved in processing visual information and recognizing objects. The model's focus on this region may suggest that it is considering the patient's ability to recognize and process visual stimuli, which could be affected in Alzheimer's Disease.\\n\\n**Conclusion**\\n\\nThe heatmap analysis provides valuable insights into the machine learning model's decision-making process. The highlighted regions suggest that the model is examining the patient's cognitive, visual, and auditory processing abilities, which are all affected in Alzheimer's Disease. The model's focus on these regions could indicate that it is looking for signs of cognitive decline, visual processing impairment, and auditory processing difficulties, which are common features of the disease.\\n\\nThese insights can enhance clinical decision-making by highlighting the importance of examining these cognitive and sensory processing abilities in patients suspected of having Alzheimer's Disease. Additionally, these findings could reveal new aspects of Alzheimer's Disease pathology and diagnosis, encouraging further research into the underlying mechanisms of the disease.\",\n",
       " 'session_id': 'OAS30788_MR_d4108'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "c7d109ec7057aa1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:18.782971Z",
     "start_time": "2024-07-01T07:23:17.386070Z"
    }
   },
   "source": [
    "if glob.glob(saved_path+ f'{name_model}.pth'):\n",
    "\tprint(f'Loading {name_model}.pth')\n",
    "\tdensenet.load_state_dict(torch.load(saved_path + f'{name_model}.pth'))\n",
    "else:\n",
    "\tprint('Train of the model')\n",
    "\ttrain_metrics = training_model(\n",
    "\t\tmodel = densenet,\n",
    "\t\tdata = [train, val],\n",
    "\t\ttransforms = [train_transform, eval_transform],\n",
    "\t\tepochs = epochs,\n",
    "\t\tdevice = get_device(),\n",
    "\t\tpaths = [saved_path, reports_path, logs_path],\n",
    "\t\tnum_workers=0,\n",
    "\t\tverbose=True\n",
    "\t)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DenseNetMM_best.pth\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "151e3073d8cfb812",
   "metadata": {},
   "source": [
    "## Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "id": "36632317f8234359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:18.785600Z",
     "start_time": "2024-07-01T07:23:18.783858Z"
    }
   },
   "source": [
    "name_fextractor = 'DenseNetMMFeatureExtractor'"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "abeaaea064344a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:18.880973Z",
     "start_time": "2024-07-01T07:23:18.788061Z"
    }
   },
   "source": [
    "VOCABULARY_SIZE = 1179\n",
    "#dimensions of the word embedding vector\n",
    "EMBEDDING_DIM = 512\n",
    "# number of units in the recurrent layers\n",
    "UNITS = 512\n",
    "#number of samples that will propagated through the network at once. \n",
    "BATCH_SIZE = 32\n",
    "#shuffling the dataset\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "def preprocess(text):\n",
    "    #conver all text into lower\n",
    "    text = text.lower()\n",
    "    #remove all character from text that are not words and whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    #replace multiple whitespace with a single space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #remove any leading or trailing whitespace from the text\n",
    "    text = text.strip()\n",
    "    #Add start and end token to the text at begining and end of the text respectively\n",
    "    text = '[start] ' + text + ' [end]'\n",
    "    return text\n",
    "\n",
    "# concat for get all df\n",
    "entire_df = train + val + test\n",
    "\n",
    "all_text = [explaination['explanation'] for explaination in entire_df]\n",
    "\n",
    "# tokenize the text\n",
    "#Keras preprocessing layer that transforms text into sequences of integers.\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    #set maximum number of tokens (words) that the tokenizer will keep\n",
    "    max_tokens=VOCABULARY_SIZE, \n",
    "    standardize=None,\n",
    "    #specifies the length of the output sequences\n",
    "    output_sequence_length=output_length\n",
    ")\n",
    "\n",
    "# Adapting the Tokenizer to all caption\n",
    "tokenizer.adapt(all_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(vocab_size))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1179\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#layer that maps strings to integer indices.\n",
    "word2idx = tf.keras.layers.StringLookup(\n",
    "    #specifies a token that will be treated as a mask\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "#The vocabulary is obtained from the tokenizer using the get_vocabulary() method, which returns a list of strings\n",
    "#representing the vocabulary in order of frequency (most frequent first).\n",
    "\n",
    "idx2word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True\n",
    ")"
   ],
   "id": "c368d06c1d6bcf52"
  },
  {
   "cell_type": "code",
   "id": "a73cbe58bf77e466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T07:23:19.090115Z",
     "start_time": "2024-07-01T07:23:18.881886Z"
    }
   },
   "source": [
    "# CNN encoder\n",
    "encoder = DenseNetMM(\n",
    "    in_channels = len(CHANNELS),\n",
    "    in_size = SIZE,\n",
    "    in_features_size= len(FEATURES),\n",
    "    out_channels = 3 if MULTICLASS else 2,\n",
    "    append_features = True,\n",
    "    name=name_fextractor\n",
    ")\n",
    "\n",
    "# Upload the previous model for the feature extraction\n",
    "if glob.glob(saved_path+ f'{name_model}.pth'):\n",
    "\tprint(f'Loading {name_model}.pth')\n",
    "\tencoder.load_state_dict(torch.load(saved_path + f'{name_model}.pth'))\n",
    "else:\n",
    "\tprint('Train of the model')\n",
    "\ttrain_metrics = training_model(\n",
    "\t\tmodel = encoder,\n",
    "\t\tdata = [train, val],\n",
    "\t\ttransforms = [train_transform, eval_transform],\n",
    "\t\tepochs = epochs,\n",
    "\t\tdevice = get_device(),\n",
    "\t\tpaths = [saved_path, reports_path, logs_path],\n",
    "\t\tnum_workers=0,\n",
    "\t\tverbose=True\n",
    "\t)\n",
    "    \n",
    "# get just the feature extractor from image\n",
    "encoder = torch.nn.Sequential(\n",
    "    encoder.features_img,\n",
    "    encoder.output_layers,\n",
    ")\n",
    "\n",
    "encoder"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DenseNetMM_best.pth\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    #Forward Pass (call method):\n",
    "    def call(self, x, training):\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        attn_output = self.attention(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=None,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        x = self.layer_norm_2(x + attn_output)\n",
    "\n",
    "        return x"
   ],
   "id": "552152b77df58ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Embeddings(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            vocab_size, embed_dim\n",
    "        )\n",
    "        \n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            max_len, embed_dim, input_shape=(None, max_len)\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        #input_ids: A tensor of token IDs representing the input sequences.\n",
    "        length = tf.shape(input_ids)[-1]\n",
    "        #A range of position IDs from 0 to length - 1 is created\n",
    "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
    "        #adds a new axis to make position_ids a batch-compatible tensor of shape\n",
    "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
    "\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        return token_embeddings + position_embeddings"
   ],
   "id": "3b12e0063048415a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Embeddings(tokenizer.vocabulary_size(), EMBEDDING_DIM, output_length)(next(iter(train))[1]).shape",
   "id": "9e10da51c9944bf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, units, num_heads):\n",
    "        super().__init__()\n",
    "# embedding layer to create token and positional embeddings.\n",
    "        self.embedding = Embeddings(\n",
    "            tokenizer.vocabulary_size(), embed_dim, output_length)\n",
    "# for self attention\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "#for attending to the encoder's output\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        #three layer normalization layers\n",
    "\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "        #Dense layers for FF network and output layer\n",
    "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
    "        #two dropout layers\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def call(self, input_ids, encoder_output, training, mask=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "\n",
    "        combined_mask = None\n",
    "        padding_mask = None\n",
    "        #Prepares the masks for attention mechanisms\n",
    "        if mask is not None:\n",
    "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "        #Applies self-attention on the embeddings\n",
    "        attn_output_1 = self.attention_1(\n",
    "            query=embeddings,\n",
    "            value=embeddings,\n",
    "            key=embeddings,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training\n",
    "        )\n",
    "        #Adds the input embeddings to the attention output and normalizes\n",
    "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
    "        #Applies attention on the encoder output (cross-attention).\n",
    "        attn_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_output,\n",
    "            key=encoder_output,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "        #Adds the previous output to the cross-attention output and normalizes\n",
    "\n",
    "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
    "        #Feedforward network and dropout\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "#creates a causal mask to ensure that each position can only attend to earlier positions and itself, preventing information leakage from future tokens\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    \n",
    "\n",
    "class ImageCaptioningModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_aug = image_aug\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
    "\n",
    "    \n",
    "    #Loss Calculation\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    \n",
    "    #This method calculates the masked loss by applying the mask to the loss values\n",
    "    #and then computing the average loss per non-padding token\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "        \n",
    "        \n",
    "    #This method calculates the masked accuracy by comparing predicted tokens to \n",
    "    # #the ground truth tokens and applying the mask\n",
    "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
    "        encoder_output = self.encoder(img_embed, training=True)\n",
    "        y_input = captions[:, :-1]\n",
    "        y_true = captions[:, 1:]\n",
    "        mask = (y_true != 0)\n",
    "        y_pred = self.decoder(\n",
    "            y_input, encoder_output, training=True, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
    "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
    "        return loss, acc\n",
    "        \n",
    "        #This method computes the loss and accuracy for a given batch by first encoding\n",
    "        # #the image embeddings, preparing the input and target sequences for the decoder, \n",
    "        # and then calculating the loss and accuracy using the decoder's predictions.\n",
    "    def train_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        if self.image_aug:\n",
    "            imgs = self.image_aug(imgs)\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, acc = self.compute_loss_and_acc(\n",
    "                img_embed, captions\n",
    "            )\n",
    "\n",
    "        train_vars = (\n",
    "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        )\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "#This method performs a training step, including optional image augmentation, \n",
    "#forward pass, loss and accuracy computation, gradient computation, and model\n",
    " # weights update using the optimizer\n",
    "\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        loss, acc = self.compute_loss_and_acc(\n",
    "            img_embed, captions, training=False\n",
    "        )\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "#This method performs an evaluation step, similar to the training step but\n",
    "#without gradient computation and weight updates.\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.acc_tracker]"
   ],
   "id": "cb7ac120a2781dc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_data(img_dict):\n",
    "    img = img_dict['data'] \n",
    "    #tokenizes the caption using the tokenizer created earlier\n",
    "    caption = tokenizer(img_dict['explanation'])\n",
    "    return img, caption"
   ],
   "id": "7d43a7d7d00013a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train))\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (val))\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ],
   "id": "843463564b67b28d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoder_transformer = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
    "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
    "\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=encoder, encoder=encoder_transformer, decoder=decoder\n",
    ")"
   ],
   "id": "5da583de2af53df8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cross_entropy \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mSparseCategoricalCrossentropy(\n\u001B[1;32m      2\u001B[0m     from_logits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      3\u001B[0m )\n\u001B[1;32m      5\u001B[0m early_stopping \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mEarlyStopping(patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      7\u001B[0m caption_model\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m      8\u001B[0m     optimizer\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(),\n\u001B[1;32m      9\u001B[0m     loss\u001B[38;5;241m=\u001B[39mcross_entropy\n\u001B[1;32m     10\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "caption_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=cross_entropy\n",
    ")"
   ],
   "id": "f24777705b511b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not glob.glob(saved_path + 'transformer_caption_model.*') == []:\n",
    "    print('Caption model training...')\n",
    "    history = caption_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    caption_model.save_weights(saved_path + 'transformer_caption_model.h5')\n",
    "else:\n",
    "    print('Loading Caption Model...')\n",
    "    caption_model.load_weights(saved_path + 'transformer_caption_model.h5')"
   ],
   "id": "ab640d433b95a42b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merging of the proposed methods",
   "id": "efe0392784b62bca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_grad_cam_explanation(image, label, pred, heatmap, mask, caption, alpha=128):\n",
    "\t\"\"\"\n",
    "\tPlots model input image, Grad-CAM heatmap, segmentation mask and the explanation generated\n",
    "\tArgs:\n",
    "\t\timage (numpy.ndarray): the input 3D image.\n",
    "\t\tlabel (int): the input image label.\n",
    "\t\tpred (int): model prediction for input image.\n",
    "\t\theatmap (numpy.ndarray): the Grad-CAM 3D heatmap.\n",
    "\t\tmask (numpy.ndarray): the computed 3D segmentation mask.\n",
    "\t\tcaption (string): the explanation generated caption.\n",
    "\t\talpha (int): transparency channel. Between 0 and 255.\n",
    "\tReturns:\n",
    "\t\tNone.\n",
    "\t\"\"\"\n",
    "\tif alpha >= 0 and alpha <= 255:\n",
    "\t\theatmap_mask = np.zeros((image.shape[0], image.shape[1], image.shape[2], 4), dtype='uint8')\n",
    "\t\theatmap_mask[mask == 1] = [255, 0, 0, alpha]\n",
    "\t\timage = image[:,:,int(image.shape[2] / 2)]\n",
    "\t\theatmap = heatmap[:,:,int(heatmap.shape[2] / 2)]\n",
    "\t\theatmap_mask = heatmap_mask[:,:,int(heatmap_mask.shape[2] / 2),:]\n",
    "\t\tfig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\t\tnorm_img = cv2.normalize(image, np.zeros((image.shape[1], image.shape[0])), 0, 1, cv2.NORM_MINMAX)\n",
    "\t\tim_shows = [\n",
    "\t\t\taxs[0].imshow(norm_img, cmap='gray', interpolation='bilinear', vmin = .0, vmax = 1.),\n",
    "\t\t\taxs[1].imshow(heatmap, cmap='jet', interpolation='bilinear', vmin = .0, vmax = 1.),\n",
    "\t\t\taxs[2].imshow(norm_img, cmap='gray', interpolation='bilinear', vmin = .0, vmax = 1.)\n",
    "\t\t]\n",
    "\t\taxs[2].imshow(heatmap_mask, interpolation='bilinear')\n",
    "\t\taxs[0].set_title('Label=' + ('NON-AD' if label == 0 else 'AD') + ' | Prediction=' + ('NON-AD' if pred == 0 else 'AD'), fontsize=16)\n",
    "\t\taxs[1].set_title('Grad-CAM Heatmap', fontsize=16)\n",
    "\t\taxs[2].set_title('Mask - Threshold ' + str(.8), fontsize=16)\n",
    "\t\tfor i, ax in enumerate(axs):\n",
    "\t\t\tax.axis('off')\n",
    "\t\t\tfig.colorbar(im_shows[i], ax=ax, ticks=np.linspace(0,1,6))\n",
    "            \n",
    "        # insert of caption generated\n",
    "        fig.text(0.5, 0.04, caption, ha='center', va='center')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "\t\tprint('\\n' + ''.join(['> ' for i in range(30)]))\n",
    "\t\tprint('\\nERROR: alpha channel \\033[95m '+alpha+'\\033[0m out of range [0,255].\\n')\n",
    "\t\tprint(''.join(['> ' for i in range(30)]) + '\\n')"
   ],
   "id": "18a2908249a121ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_results_and_plot(image_dict, predictor, generator,saved_path, plot=False):\n",
    "    '''\n",
    "    In this function we predict the class of the image after that we\n",
    "    keep the Grad-CAM and the explanation and return them\n",
    "    :param plot: if True we plot the Grad-CAM and explanation\n",
    "    :param saved_path: directory where the model weights are stored\n",
    "    :param image_dict: image dictionary\n",
    "    :param predictor: model for predict all value\n",
    "    :param generator: model for the generation of explanation\n",
    "    :param tokenizer: tokenizer object\n",
    "    :param max_length: maximum length of explanation\n",
    "    :return: \n",
    "    '''\n",
    "    \n",
    "    # Keep the image, the mask and the prediction\n",
    "    image, mask, pred, label, heatmap = get_gradcam(\n",
    "        example=image_dict,\n",
    "        model=predictor,\n",
    "        saved_path=saved_path,\n",
    "        threshold=.8,\n",
    "    )\n",
    "    \n",
    "    # Generate the description from the processed image\n",
    "    explanation = generator.predict(image)\n",
    "    \n",
    "    if plot:\n",
    "        plot_grad_cam_explanation(image, label, pred, heatmap, mask, explanation)\n",
    "    \n",
    "    return  image, label, pred, heatmap, mask, explanation"
   ],
   "id": "6dc33693af7c4208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# get a random example from the entire dataset\n",
    "example = entire_df[random.randint(0, entire_df.shape[0]-1)]\n",
    "get_results_and_plot(\n",
    "    image_dict=example,\n",
    "    predictor=densenet,\n",
    "    generator=caption_model,\n",
    "    saved_path=saved_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=output_length,\n",
    "    plot=True\n",
    ")"
   ],
   "id": "156869a8a6cbe018"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3a163c23e063c236"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
